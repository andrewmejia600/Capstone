---
title: "CapStone"
author: "Laura Lazarescou, Tina Pai, Andrew Mejia, Sabrina Purvis"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 6
  github_document:
    toc: yes
    toc_depth: 6
  word_document:
    toc: yes
    toc_depth: '6'
always_allow_html: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#http://rstudio-pubs-static.s3.amazonaws.com/336732_52d1b0e682634b5eae42cf86e1fc2a98.html 

```{r library, include=FALSE, warning=FALSE, options("rgdal_show_exportToProj4_warnings"="none")}
library(tidyverse)
library(rpart)
library(tm)
library(tidyverse)
library(randomForest)
library(caret)
library(knitr)
library(XML)
library(stringr)
library(dplyr)
library(corrplot)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(dplyr)
library(ggraph)
library(igraph)
library(e1071)
library(ROCR)
library(DiagrammeR)
library(doSNOW)
library(parallel)
library(ggplot2)
library(xgboost)
library(rgdal)
library(rgeos)
library(raster)
library(sp)
library(ggmap)
library(ROCR)
library(mlr)
library(parallelMap)
```

```{r}
setwd("/home/andrew/Desktop/Dallas Files/")
file_direct = '/home/andrew/Desktop/Dallas Files/GIS_PACKAGE_FILES_TO_CSV/' 

#Shape layer of all parcels in the City of Dallas
clipped_parcels_by_dallas_geomtry = read.csv(file=paste0(file_direct,'Clipped_Parcels_by_Dallas_Simple.csv'))

#All of Dallas County Parcels 
DCAD_aprl_year = read.table(unz("DCAD2019_CURRENT.ZIP", "account_apprl_year.csv"), sep = ",", header = TRUE)

#These are the only accounts in the Dallas city limits 
DCAD_aprl_year_filtered_by_clipped_parcels = clipped_parcels_by_dallas_geomtry %>% inner_join(DCAD_aprl_year, by = c("Acct"="ACCOUNT_NUM"))

dim(DCAD_aprl_year[duplicated(DCAD_aprl_year$ACCOUNT_NUM),])[1]
#You would want to use caution eliminate the parcels since these are multi-use buildings 
#00000300268000000 with account 0000030026800TE00	
dim(DCAD_aprl_year[duplicated(DCAD_aprl_year$GIS_PARCEL_ID),])[1]
head(DCAD_aprl_year[duplicated(DCAD_aprl_year$GIS_PARCEL_ID),], n = 20)

###CPAL Annotations
CPAL_Label_annotations = read.csv(file=paste0(file_direct,'Clipped_Parcels_by_Dallas_Simple_vac_pts_annotations.csv'))
CPAL_Label_annotations["VAC_PAR"] = 1

dim(CPAL_Label_annotations [duplicated(CPAL_Label_annotations $ACCOUNT_NUM),])[1]
#You would want to use caution eliminate the parcels since these are multi-use buildings 
#00000300268000000 with account 0000030026800TE00	
dim(CPAL_Label_annotations [duplicated(CPAL_Label_annotations $GIS_PARCEL_ID),])[1]
head(CPAL_Label_annotations [duplicated(CPAL_Label_annotations $GIS_PARCEL_ID),], n = 20)

### Join to DCAD FILTERED BY PARCELS TO CPAL ANNOTATIONS, ACCT has two accounts see "00000412513000000" 
DCAD_aprl_year_filtered_by_clipped_parcels_annotated = left_join(DCAD_aprl_year_filtered_by_clipped_parcels,CPAL_Label_annotations, by = c("Acct" = "ACCOUNT_NUM"), suffix = c(".x", ".y"), keep = TRUE)

### Add remaing target labels
DCAD_aprl_year_filtered_by_clipped_parcels_annotated['VAC_PAR'] = ifelse(is.na(DCAD_aprl_year_filtered_by_clipped_parcels_annotated$VAC_PAR),0, DCAD_aprl_year_filtered_by_clipped_parcels_annotated$VAC_PAR) 

#Size of not vacant
dim(DCAD_aprl_year_filtered_by_clipped_parcels_annotated %>% filter(VAC_PAR == 0))
#Size of Vacant
dim(DCAD_aprl_year_filtered_by_clipped_parcels_annotated %>% filter(VAC_PAR == 1))

#size of CPAL annotations
dim(CPAL_Label_annotations)

### 311 
Clipped_311_2019 = read.csv(file=paste0(file_direct,'Clipped_Parcels_by_Dallas_Simple_inner_join_to_Clipped_2019_311.csv'))

DCAD_aprl_year_filtered_by_clipped_parcels_annotated_joined_to_311 = left_join(DCAD_aprl_year_filtered_by_clipped_parcels_annotated,Clipped_311_2019,by = c("Acct.x" = "Acct"), suffix = c(".x", ".y"), keep = TRUE )
  
#### Crime 
Clipped_All_Crime_2019 = read.csv(file=paste0(file_direct,'Clipped_Parcels_by_Dallas_Simple_inner_join_to_Clipped_2019_Police_Incidents.csv'))

### CO 
Clipped_CO_2019 = read.csv(file=paste0(file_direct,'Clipped_Parcels_by_Dallas_Simple_inner_join_to_Clipped_2019_co.csv'))
Clipped_CO_2019['CO_ON_ACCOUNT'] = 1

DCAD_aprl_year_filtered_by_clipped_parcels_annotated_joined_to_co = left_join(DCAD_aprl_year_filtered_by_clipped_parcels_annotated,Clipped_CO_2019,by = c("Acct.x" = "Acct"), suffix = c(".x", ".y"), keep = TRUE )

DCAD_aprl_year_filtered_by_clipped_parcels_annotated_joined_to_co['CO_ON_ACCOUNT'] = ifelse(is.na(DCAD_aprl_year_filtered_by_clipped_parcels_annotated_joined_to_co$CO_ON_ACCOUNT), 0, 1)

#Get a count of accounts with a CO
Account_counts = DCAD_aprl_year_filtered_by_clipped_parcels_annotated_joined_to_co %>% group_by(Acct.x) %>% tally(CO_ON_ACCOUNT)

### Test data for models 

DCAD_test_set = DCAD_aprl_year_filtered_by_clipped_parcels_annotated %>% left_join(Account_counts, by = c("Acct.x" = "Acct.x"), suffix = c(".u", ".z"), keep = TRUE  )



myvars <- c("LAND_VAL", "TOT_VAL", "n", "VAC_PAR", "Acct.x.u")
#myvars <- c("LAND_VAL", "TOT_VAL", "n", "VAC_PAR")

data = DCAD_test_set[myvars]

ambigious_accounts = list(unique(data[duplicated(data$Acct.x.u),5]))

data = data[!duplicated(data$Acct.x.u),]

data = data[,c(1:4)]

#re-index after selecting and removing duplicated account numbers
row.names(data) = NULL

#data['VAC_PAR_L'] = as.factor(ifelse(data['VAC_PAR'] == 1,"True", "False"))

```

#80/20 train test stratified split 
#https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/createDataPartition
```{r}
rand_seed = 42
set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data$VAC_PAR,
  p = .70,
  list = FALSE
)
train = data[train_partition,]
test =  data[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)
```

```{r}
rand_seed = 42
set.seed(rand_seed)
# create baseline random forest model
random_forest_1 <- randomForest(VAC_PAR ~., data = train, ntree = 50, importance=TRUE, na.action = na.roughfix, maxnodes = 10)
preds_1 = predict(random_forest_1,test)
preds_1_cut = ifelse(preds_1>.5,1,0)
confusionMatrix(as.factor(preds_1_cut),as.factor(test$VAC_PAR), positive = "1")
F_meas(as.factor(preds_1_cut),as.factor(test$VAC_PAR))
```


```{r}
train["VAC_PAR"] = as.character(ifelse(train["VAC_PAR"]==1,"T", "F"))
test["VAC_PAR"] = as.character(ifelse(test["VAC_PAR"]==1, "T", "F"))
traintask <- makeClassifTask (data = train,target = "VAC_PAR")
testtask <- makeClassifTask (data = test,target = "VAC_PAR")

```

```{r}
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.1)

params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree")), makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L), makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))

rdesc <- makeResampleDesc("CV",stratify = T,iters=3L)

ctrl <- makeTuneControlRandom(maxit = 5L)
```

```{r}
parallelStartSocket(cpus=detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
```
```{r}
mytune$y
```

```{r}
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
```

```{r}
xgmodel <- train(learner = lrn_tune,task = traintask)
```

```{r}
xgpred = predict(xgmodel, testtask)
```

```{r}
confusionMatrix(xgpred$data$response,xgpred$data$truth, positive = "T")
```

```{r}
parallelStop() 
```

```{r}
rand_seed = 42
set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data$VAC_PAR,
  p = .70,
  list = FALSE
)
train = data[train_partition,]
test =  data[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)
```

```{r}
data_train = as.matrix(train[,c(1:3)])
data_train_l = as.matrix(train[,c(4)])
dtrain = xgb.DMatrix(data = data_train, label = data_train_l)

data_test = as.matrix(test[,c(1:3)])
data_test_l = as.matrix(test[,c(4)])
dtest = xgb.DMatrix(data = data_test, label = data_test_l  )

```

```{r}
rand_seed = 42
params = list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, max_depth = 5, min_child_weight=8.34, subsample=0.963, colsample_bytree=0.787)
xgboost_best = xgb.train(params = params, data = dtrain, nrounds = 79, watchlist = list(val = dtest, train = dtrain), print_every_n = 10, early_stopping_rounds = 10, maximize = F, eval_metric = "error")
```
```{r}
xgbpred_best = predict(xgboost_best,dtest)
xgbpred_best_cut =  ifelse(xgbpred_best > 0.50,1,0)
```

```{r}
confusionMatrix(as.factor(xgbpred_best_cut), as.factor(data_test_l), positive = "1")
F_meas(as.factor(xgbpred_best_cut),as.factor(data_test_l))
```

```{r}
xgb.importance(feature_names = colnames(train[,c(1:3)]), model = xgboost_best, data=train[,c(1:3)], label=train[,4])
```

```{r}
xgb.plot.importance(xgb.importance(feature_names = colnames(train[,c(1:3)]), model = xgboost_best, data=train[,c(1:3)], label=train[,4]), top_n = 10)
```

```{r}
#generage ROC curve
myPred = prediction(xgbpred_best,test[,4])
perf = ROCR::performance(myPred,"tpr","fpr")
#calculate AUC
auc = ROCR::performance(myPred, measure="auc")
auc_score = auc@y.values[[1]]

#plot the curve
plot(perf,main=paste0("XGBoost ROC curve: AUC= ",auc_score), xlim=c(0,0.95), ylim=c(.55,1),colorize=TRUE)

```

