---
title: "CapStone"
author: "Laura Lazarescou, Tina Pai, Andrew Mejia, Sabrina Purvis"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 6
  github_document:
    toc: yes
    toc_depth: 6
  word_document:
    toc: yes
    toc_depth: '6'
  pdf_document:
    toc: yes
    toc_depth: '6'
always_allow_html: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#http://rstudio-pubs-static.s3.amazonaws.com/336732_52d1b0e682634b5eae42cf86e1fc2a98.html 

```{r library, include=FALSE, warning=FALSE, options("rgdal_show_exportToProj4_warnings"="none")}

library(randomForest)
library(caret)
library(e1071)
library(parallel)
library(ggplot2)
library(xgboost)
library(ROCR)
library(mlr)
library(parallelMap)
```

```{r}

read_data = read.csv('/users/mejiaa/CAPSTONE/Data/df_log_and_scaled.csv')
data = do.call(data.frame,lapply(read_data, function(x) replace(x, is.infinite(x),0)))

data = data[,c(2:24)]
colnames(data)[23] <- "VAC_PAR"
```

#80/20 train test stratified split 
#https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/createDataPartition
```{r}
rand_seed = 959
set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data$VAC_PAR,
  p = .70,
  list = FALSE
)
train = data[train_partition,]
test =  data[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)
```

```{r}
rand_seed = 959
set.seed(rand_seed)
# create baseline random forest model
random_forest_1 <- randomForest(VAC_PAR ~., data = train, ntree = 50, importance=TRUE, na.action = na.roughfix, maxnodes = 10)
preds_1 = predict(random_forest_1,test)
preds_1_cut = ifelse(preds_1>.5,1,0)
confusionMatrix(as.factor(preds_1_cut),as.factor(test$VAC_PAR), positive = "1")
F_meas(as.factor(preds_1_cut),as.factor(test$VAC_PAR))
```

```{r}
rand_seed = 42
set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data$VAC_PAR,
  p = .70,
  list = FALSE
)
train = data[train_partition,]
test =  data[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)
```


```{r}
train["VAC_PAR"] = as.character(ifelse(train["VAC_PAR"]==1,"T", "F"))
test["VAC_PAR"] = as.character(ifelse(test["VAC_PAR"]==1, "T", "F"))
traintask <- makeClassifTask (data = train,target = "VAC_PAR")
testtask <- makeClassifTask (data = test,target = "VAC_PAR")

```

```{r}
lrn <- makeLearner("classif.xgboost",predict.type = "response")
lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.1)

params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree")), makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L), makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))

rdesc <- makeResampleDesc("CV",stratify = T,iters=10L)

ctrl <- makeTuneControlRandom(maxit = 10L)
```

```{r}
parallelStartSocket(cpus=detectCores())
mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
```

```{r}
mytune$y
```

```{r}
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
```

```{r}
xgmodel <- train(learner = lrn_tune,task = traintask)
```

```{r}
xgpred = predict(xgmodel, testtask)
parallelStop()

```

```{r}
confusionMatrix(xgpred$data$response,xgpred$data$truth, positive = "T")
```



```{r}
rand_seed = 959
set.seed(rand_seed)
train_partition =  createDataPartition(
  y= data$VAC_PAR,
  p = .70,
  list = FALSE
)
train = data[train_partition,]
test =  data[-train_partition,]
print("Number of records in Training data")
nrow(train)
print("Number of records in Testing data")
nrow(test)
```

```{r}
data_train = as.matrix(train[,c(1:22)])
data_train_l = as.matrix(train[,c(23)])
dtrain = xgb.DMatrix(data = data_train, label = data_train_l)

data_test = as.matrix(test[,c(1:22)])
data_test_l = as.matrix(test[,c(23)])
dtest = xgb.DMatrix(data = data_test, label = data_test_l  )

```

```{r}
rand_seed = 959
params = list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, max_depth = 7, min_child_weight=5.26, subsample=0.653, colsample_bytree=0.77)
xgboost_best = xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(val = dtest, train = dtrain), print_every_n = 10, early_stopping_rounds = 10, maximize = F, eval_metric = "error")
```
```{r}
xgbpred_best = predict(xgboost_best,dtest)
xgbpred_best_cut =  ifelse(xgbpred_best > 0.50,1,0)
```

```{r}
confusionMatrix(as.factor(xgbpred_best_cut), as.factor(data_test_l), positive = "1")
F_meas(as.factor(xgbpred_best_cut),as.factor(data_test_l))
```

```{r}
xgb.importance(feature_names = colnames(test[,c(1:22)]), model = xgboost_best, data=test[,c(1:22)], label=test[,23])
```

```{r}
xgb.plot.importance(xgb.importance(feature_names = colnames(test[,c(1:22)]), model = xgboost_best, data=test[,c(1:22)], label=test[,23]), top_n = 12)
```

```{r}
#generage ROC curve
myPred = prediction(xgbpred_best,test[,23])
perf = ROCR::performance(myPred,"tpr","fpr")
#calculate AUC
auc = ROCR::performance(myPred, measure="auc")
auc_score = auc@y.values[[1]]

#plot the curve
plot(perf,main=paste0("XGBoost ROC curve: AUC= ",auc_score), xlim=c(0,0.95), ylim=c(.55,1),colorize=TRUE)

```

