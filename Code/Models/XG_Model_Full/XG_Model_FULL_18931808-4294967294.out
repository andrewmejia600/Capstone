
R version 4.0.2 (2020-06-22) -- "Taking Off Again"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> if (!require(caret)) install.packages('caret')
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
> library(caret)
> 
> if (!require(parallel)) install.packages('parallel')
Loading required package: parallel
> library(parallel)
> 
> if (!require(e1071)) install.packages('e1071')
Loading required package: e1071
> library(e1071)
> 
> if (!require(ggplot2)) install.packages('ggplot2')
> library(ggplot2)
> 
> if (!require(ROCR)) install.packages('ROCR')
Loading required package: ROCR
> library(ROCR)
> 
> if (!require(mlr)) install.packages('mlr')
Loading required package: mlr
Loading required package: ParamHelpers
Warning message: 'mlr' is in 'maintenance-only' mode since July 2019.
Future development will only happen in 'mlr3'
(<https://mlr3.mlr-org.com>). Due to the focus on 'mlr3' there might be
uncaught bugs meanwhile in {mlr} - please consider switching.

Attaching package: ‘mlr’

The following object is masked from ‘package:ROCR’:

    performance

The following object is masked from ‘package:e1071’:

    impute

The following object is masked from ‘package:caret’:

    train

> library(mlr)
> 
> if (!require(parallelMap)) install.packages('parallelMap')
Loading required package: parallelMap
> library(parallelMap)
> 
> if (!require(xgboost)) install.packages('xgboost')
Loading required package: xgboost
> library(xgboost)
> 
> 
> #read_data = read.csv('/users/mejiaa/CAPSTONE/Data/df_log_and_scaled.csv')
> read_data = read.csv('https://raw.githubusercontent.com/andrewmejia600/Capstone/main/Data/df_log_and_scaled.csv')
> #data = do.call(data.frame,lapply(read_data, function(x) replace(x, is.infinite(x),0)))
> 
> data = read_data
> colnames(data)[1] = "VAC_PAR"
> 
> #Simpler Model
> #data = data[,c(1,3,6,7,8,9)]
> 
> 
> #######################################################################################################################
> # read_data = read.csv('https://raw.githubusercontent.com/andrewmejia600/Capstone/Andrew/Data/df_log_and_scaled.csv')##
> # 
> # colnames(read_data)[1] = "VAC_PAR"                                                                                 ##
> # 
> # data_1 = read_data                                                                                                 ##  
> # 
> # data = read_data                                                                                                   ##
> # 
> # dmy = dummyVars(~division_cd, data = data, fullRank = T)                                                           ##
> # dat_transformed = data.frame(predict(dmy, newdata = data_1))                                                       ##
> # data = cbind(data, dat_transformed)                                                                                ##
> # 
> # 
> # dmy = dummyVars(~sptd_code, data = data, fullRank = T)                                                             ##
> # dat_transformed = data.frame(predict(dmy, newdata = data_1))                                                       ##
> # data = cbind(data, dat_transformed)                                                                                ##  
> # 
> # dmy = dummyVars(~zoning_buckets, data = data, fullRank = T)                                                        ##
> # dat_transformed = data.frame(predict(dmy, newdata = data_1))                                                       ##  
> # data = cbind(data, dat_transformed)                                                                                ##
> # 
> # data = data[,c(-2,-3,-5)]                                                                                          ##
> 
> #######################################################################################################################
> 
> rand_seed = 42
> set.seed(rand_seed)
> train_partition =  createDataPartition(
+   y= data$VAC_PAR,
+   p = .70,
+   list = FALSE
+ )
> train = data[train_partition,]
> test =  data[-train_partition,]
> print("Number of records in Training data")
[1] "Number of records in Training data"
> nrow(train)
[1] 205114
> print("Number of records in Testing data")
[1] "Number of records in Testing data"
> nrow(test)
[1] 87906
> 
> 
> ##### Create best tune using all features 
> train["VAC_PAR"] = as.character(ifelse(train["VAC_PAR"]==1,"T", "F"))
> test["VAC_PAR"] = as.character(ifelse(test["VAC_PAR"]==1, "T", "F"))
> traintask <- makeClassifTask (data = train,target = "VAC_PAR")
> testtask <- makeClassifTask (data = test,target = "VAC_PAR")
> 
> lrn <- makeLearner("classif.xgboost",predict.type = "response")
> lrn$par.vals <- list( objective="binary:logistic", eval_metric="error", nrounds=100L, eta=0.1)
> 
> params <- makeParamSet( makeDiscreteParam("booster",values = c("gbtree")), makeIntegerParam("max_depth",lower = 3L,upper = 10L), makeNumericParam("min_child_weight",lower = 1L,upper = 10L), makeNumericParam("subsample",lower = 0.5,upper = 1), makeNumericParam("colsample_bytree",lower = 0.5,upper = 1))
> 
> rdesc <- makeResampleDesc("CV",stratify = T,iters=10L)
> 
> 
> 
> ctrl <- makeTuneControlRandom(maxit = 10L)
> 
> parallelStartSocket(cpus=detectCores())
Starting parallelization in mode=socket with cpus=36.
> mytune <- tuneParams(learner = lrn, task = traintask, resampling = rdesc, measures = acc, par.set = params, control = ctrl, show.info = T)
[Tune] Started tuning learner classif.xgboost for parameter set:
                     Type len Def   Constr Req Tunable Trafo
booster          discrete   -   -   gbtree   -    TRUE     -
max_depth         integer   -   -  3 to 10   -    TRUE     -
min_child_weight  numeric   -   -  1 to 10   -    TRUE     -
subsample         numeric   -   - 0.5 to 1   -    TRUE     -
colsample_bytree  numeric   -   - 0.5 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: -0
Exporting objects to slaves for mode socket: .mlr.slave.options
Mapping in parallel: mode = socket; level = mlr.tuneParams; cpus = 36; elements = 10.
[Tune] Result: booster=gbtree; max_depth=10; min_child_weight=4.9; subsample=0.668; colsample_bytree=0.944 : acc.test.mean=0.9836286
> 
> mytune$y
acc.test.mean 
    0.9836286 
> 
> lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)
> 
> xgmodel <- train(learner = lrn_tune,task = traintask)
[1]	train-error:0.017015 
[2]	train-error:0.016732 
[3]	train-error:0.016615 
[4]	train-error:0.016610 
[5]	train-error:0.016557 
[6]	train-error:0.016498 
[7]	train-error:0.016318 
[8]	train-error:0.016298 
[9]	train-error:0.016054 
[10]	train-error:0.016020 
[11]	train-error:0.016054 
[12]	train-error:0.015991 
[13]	train-error:0.015986 
[14]	train-error:0.015864 
[15]	train-error:0.015767 
[16]	train-error:0.015894 
[17]	train-error:0.015855 
[18]	train-error:0.015762 
[19]	train-error:0.015713 
[20]	train-error:0.015669 
[21]	train-error:0.015664 
[22]	train-error:0.015650 
[23]	train-error:0.015582 
[24]	train-error:0.015518 
[25]	train-error:0.015494 
[26]	train-error:0.015367 
[27]	train-error:0.015309 
[28]	train-error:0.015221 
[29]	train-error:0.015270 
[30]	train-error:0.015245 
[31]	train-error:0.015221 
[32]	train-error:0.015016 
[33]	train-error:0.015011 
[34]	train-error:0.014992 
[35]	train-error:0.014967 
[36]	train-error:0.014982 
[37]	train-error:0.014948 
[38]	train-error:0.014919 
[39]	train-error:0.014884 
[40]	train-error:0.014938 
[41]	train-error:0.014860 
[42]	train-error:0.014865 
[43]	train-error:0.014865 
[44]	train-error:0.014884 
[45]	train-error:0.014899 
[46]	train-error:0.014870 
[47]	train-error:0.014831 
[48]	train-error:0.014797 
[49]	train-error:0.014787 
[50]	train-error:0.014772 
[51]	train-error:0.014767 
[52]	train-error:0.014748 
[53]	train-error:0.014660 
[54]	train-error:0.014636 
[55]	train-error:0.014636 
[56]	train-error:0.014626 
[57]	train-error:0.014631 
[58]	train-error:0.014519 
[59]	train-error:0.014563 
[60]	train-error:0.014543 
[61]	train-error:0.014436 
[62]	train-error:0.014421 
[63]	train-error:0.014368 
[64]	train-error:0.014368 
[65]	train-error:0.014309 
[66]	train-error:0.014280 
[67]	train-error:0.014275 
[68]	train-error:0.014265 
[69]	train-error:0.014251 
[70]	train-error:0.014226 
[71]	train-error:0.014221 
[72]	train-error:0.014134 
[73]	train-error:0.014114 
[74]	train-error:0.014099 
[75]	train-error:0.014085 
[76]	train-error:0.014090 
[77]	train-error:0.014060 
[78]	train-error:0.014056 
[79]	train-error:0.014017 
[80]	train-error:0.014026 
[81]	train-error:0.014002 
[82]	train-error:0.013958 
[83]	train-error:0.013914 
[84]	train-error:0.013895 
[85]	train-error:0.013865 
[86]	train-error:0.013841 
[87]	train-error:0.013856 
[88]	train-error:0.013812 
[89]	train-error:0.013802 
[90]	train-error:0.013768 
[91]	train-error:0.013748 
[92]	train-error:0.013709 
[93]	train-error:0.013705 
[94]	train-error:0.013690 
[95]	train-error:0.013646 
[96]	train-error:0.013661 
[97]	train-error:0.013636 
[98]	train-error:0.013597 
[99]	train-error:0.013617 
[100]	train-error:0.013612 
> 
> xgpred = predict(xgmodel, testtask)
> parallelStop()
Stopped parallelization. All cleaned up.
> 
> 
> confusionMatrix(xgpred$data$response,xgpred$data$truth, positive = "T")
Confusion Matrix and Statistics

          Reference
Prediction     F     T
         F 78354   440
         T  1083  8029
                                          
               Accuracy : 0.9827          
                 95% CI : (0.9818, 0.9835)
    No Information Rate : 0.9037          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9038          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.94805         
            Specificity : 0.98637         
         Pos Pred Value : 0.88115         
         Neg Pred Value : 0.99442         
             Prevalence : 0.09634         
         Detection Rate : 0.09134         
   Detection Prevalence : 0.10366         
      Balanced Accuracy : 0.96721         
                                          
       'Positive' Class : T               
                                          
> 
> #### Generate new seed for test train split for actual model from best tune 
> rand_seed = 959
> set.seed(rand_seed)
> train_partition =  createDataPartition(
+   y= data$VAC_PAR,
+   p = .70,
+   list = FALSE
+ )
> train = data[train_partition,]
> test =  data[-train_partition,]
> print("Number of records in Training data")
[1] "Number of records in Training data"
> nrow(train)
[1] 205114
> print("Number of records in Testing data")
[1] "Number of records in Testing data"
> nrow(test)
[1] 87906
> 
> ####### Train Features 
> data_train = as.matrix(train[,c(2:21)])
> ####### Train Target Labels
> data_train_l = as.matrix(train[,c(1)])
> dtrain = xgb.DMatrix(data = data_train, label = data_train_l)
> 
> ###### Test Features
> data_test = as.matrix(test[,c(2:21)])
> ###### Test Target Labels
> data_test_l = as.matrix(test[,c(1)])
> dtest = xgb.DMatrix(data = data_test, label = data_test_l  )
> 
> rand_seed = 959
> params = list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, max_depth = 7, min_child_weight=5.26, subsample=0.653, colsample_bytree=0.77)
> xgboost_best = xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(val = dtest, train = dtrain), print_every_n = 10, early_stopping_rounds = 10, maximize = F, eval_metric = "error")
[1]	val-error:0.025129	train-error:0.025732 
Multiple eval metrics are present. Will use train_error for early stopping.
Will train until train_error hasn't improved in 10 rounds.

[11]	val-error:0.018827	train-error:0.018687 
[21]	val-error:0.017758	train-error:0.017537 
[31]	val-error:0.017462	train-error:0.017020 
[41]	val-error:0.017246	train-error:0.016620 
[51]	val-error:0.017234	train-error:0.016298 
[61]	val-error:0.016973	train-error:0.016064 
[71]	val-error:0.016984	train-error:0.015991 
[81]	val-error:0.017007	train-error:0.015752 
[91]	val-error:0.016950	train-error:0.015489 
[100]	val-error:0.016711	train-error:0.015289 
> 
> xgbpred_best = predict(xgboost_best,dtest)
> xgbpred_best_cut =  ifelse(xgbpred_best > 0.50,1,0)
> 
> confusionMatrix(as.factor(xgbpred_best_cut), as.factor(data_test_l), positive = "1")
Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 78216   395
         1  1074  8221
                                          
               Accuracy : 0.9833          
                 95% CI : (0.9824, 0.9841)
    No Information Rate : 0.902           
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9087          
                                          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.95416         
            Specificity : 0.98645         
         Pos Pred Value : 0.88445         
         Neg Pred Value : 0.99498         
             Prevalence : 0.09801         
         Detection Rate : 0.09352         
   Detection Prevalence : 0.10574         
      Balanced Accuracy : 0.97030         
                                          
       'Positive' Class : 1               
                                          
> F_meas(as.factor(xgbpred_best_cut),as.factor(data_test_l))
[1] 0.9906967
> 
> xgb.importance(feature_names = colnames(test[,c(2:21)]), model = xgboost_best, data=test[,c(2:21)], label=test[,1])
                               Feature         Gain        Cover    Frequency
 1:                     zoning_buckets 2.636738e-01 0.0824346236 0.0474222319
 2:                        log_tot_val 1.800582e-01 0.1610197086 0.1213835110
 3:                    num_division_cd 1.337174e-01 0.0422760941 0.0163149880
 4:                           num_sptd 1.060550e-01 0.0851165955 0.0363280400
 5:                        permit_type 1.001341e-01 0.0456131739 0.0365455732
 6:                       log_land_val 6.363245e-02 0.1727781733 0.1744616054
 7:                        num_nbhd_cd 4.388959e-02 0.1383863055 0.1603219491
 8:                      log_area_sqft 4.274677e-02 0.0939208918 0.1905590603
 9:                       log_impr_val 4.213291e-02 0.1029381989 0.0952795301
10:           days_since_permit_scaled 1.226059e-02 0.0382217436 0.0750489450
11:              count_of_crime_scaled 7.382000e-03 0.0097051226 0.0130519904
12:               count_permits_scaled 3.749345e-03 0.0137725584 0.0258864477
13:            days_since_issue_scaled 2.098065e-04 0.0046352940 0.0028279313
14: days_from_CO_appro_to_issue_scaled 1.478189e-04 0.0035123430 0.0019577986
15:                   count_COs_scaled 7.495322e-05 0.0015348968 0.0006525995
16:                count_of_311_scaled 4.725101e-05 0.0010597806 0.0006525995
17:                     CO_sqft_scaled 4.706244e-05 0.0016902812 0.0008701327
18:                      CO_code_distr 2.563669e-05 0.0006524320 0.0002175332
19:                            CO_type 1.531593e-05 0.0007317828 0.0002175332
Warning message:
In xgb.importance(feature_names = colnames(test[, c(2:21)]), model = xgboost_best,  :
  xgb.importance: parameters 'data', 'label' and 'target' are deprecated
> 
> xgb.plot.importance(xgb.importance(feature_names = colnames(test[,c(2:21)]), model = xgboost_best, data=test[,c(2:21)], label=test[,1]), top_n = 12)
Warning message:
In xgb.importance(feature_names = colnames(test[, c(2:21)]), model = xgboost_best,  :
  xgb.importance: parameters 'data', 'label' and 'target' are deprecated
> 
> 
> #generate ROC curve
> myPred = prediction(xgbpred_best,test[,1])
> 
> 
> 
> perf = ROCR::performance(myPred,"tpr","fpr")
> #calculate AUC
> auc = ROCR::performance(myPred, measure="auc")
> auc_score = auc@y.values[[1]]
> 
> #plot the curve
> plot(perf,main=paste0("XGBoost ROC curve: AUC= ",auc_score), xlim=c(0,0.95), ylim=c(.55,1),colorize=TRUE)
> 
> 
> test['Tuned_XG_Preds'] = xgbpred_best
> test['Tuned_XG_preds_Cut'] = xgbpred_best_cut
> 
> 
> write.csv(test, 'XG_test_out.csv')
> 
> 
> xgb.save(xgboost_best, "xgbFinal")
[1] TRUE
> saveRDS(xgboost_best, "./XG_final_model.rds")
> 
